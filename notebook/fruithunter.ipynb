{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fruithunter pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The fruithunter pipeline was developed to segment and count apples from point clouds resulting of scans of apples orchards. Different measurement protocols were considered for orchard scans. As illustrated in figure 1, two main protocol were considered. In the first one, scans are positionned every five trees in the middle of the surounding rows. In the second protocol, scans are performed from close positions in the left and right rows of each tree. The protocols are named low and high resolution protocols respectivelly.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/protocols.png\"\n",
    "         alt=\"Protocols\" \n",
    "         width=\"400\" \n",
    "         height=\"500\">\n",
    "    <figcaption>Figure 1. Field measurement protocols schematic. Green circles labeled with a \"T\" represent the apple trees. Gray circles labelled with an \"S\" represent the sensor position at the moment of the measurement. The low and hight resolution protocol are presented in the left and right part of the figure respectivelly. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Each protocol set different levels of precision in the description of the apples trees. In the low resolution protocol, a gradient of densities can be observed that show decrease of density from the scan positions to trees at longest distances from the scan positions as shown in Figure 2. In this figure, density gradient is represented by colors with blue representing lowest density and red highest one. In the high resolution protocol, all trees have similar point densities. The scans contains different levels of noise due to different phenomena. First, occlusion between elements create missing data. Second, outliers are created from multiple hits from the same lidar beam creating intermediate points. Environmental conditions (wind, lighting) can also create perturbation in the measurement.\n",
    "\n",
    "By default, X,Y,Z coordinates are infered from Lidar scans but other radiometric features can be considered. In our case, from real scans, reflectance, deviation and amplitude of the signal can be considered for each points. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/density.png\"\n",
    "         alt=\"Protocols\" \n",
    "         width=\"400\" \n",
    "         height=\"500\">\n",
    "    <figcaption>Figure 2. Point cloud density for the low resolution protocol</figcaption>\n",
    "</figure>\n",
    "\n",
    "From this noisy data set, it is not possible to define a simple set of rules to segment apples. To handle this complex signal, different approaches from machine and deep learning are going to be used. The first one will use random forest as a segmentation module and will use the FPFH as a feature descriptor. The second approach will use RandLA-NET, a deep learning model specially designed to consume point clouds with millions of points  and make segmentation of multiple elements in a efficient way. The global workflow of the pipeline could bee see it on figure 3. The Random Forest and RandLA-NET models will be detailled in the following sections.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/fruithunter.png\"\n",
    "         alt=\"Fruithunter Pipeline\" \n",
    "         width=\"600\" \n",
    "         height=\"700\">\n",
    "    <figcaption>Figure 3. The fruithunter pipeline</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Random Forest \n",
    "\n",
    "Random Forest is an algorithm composed by <b>N</b> blocks of decision trees. A decision tree will use a feature map and will generate a series of questions to infer the class labels of the sampled points. The idea behind this procedure is to split the data on the features that returns the biggest values for the information gain (IG). The feature splitting will continue until each final node belongs to the same class. To avoid over hitting of the model, decision trees are pruned according to a predefined maximal depth.  The idea of using several decision trees is to minimise the variance of the model without increasing the bias. In general, random forest are considered to have good classification perfomance and scalability. \n",
    "\n",
    "In this project random forests are fitted with 33 features (FPFH) or 36 if the radiometric features are added, the model is composed by 300 decision trees and the depth of each decision tree is 15 nodes. \n",
    "\n",
    "### RandLA-NET\n",
    "\n",
    "RandLA-NET is a state-of-the-art deep learning model developed to consume raw point clouds of large size (>1M) and perform segmentation with good accuracy. In order to achieve this objective, 3 new layers were developed as illustrated in figure 4.   \n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/rdnet_layers.png\"\n",
    "         alt=\"rdnet layers\" \n",
    "         width=\"600\" \n",
    "         height=\"700\">\n",
    "    <figcaption>Figure 4. RandLA-NET main Layers</figcaption>\n",
    "</figure>\n",
    "\n",
    "The <i>local spatial encoding</i> is applied on each point and determine and encode the relative XYZ coordinates of all the neighbouring points with additionnal point features. Particularly this layer allows the network to abstract complex geometries. The <i>Attentive Pooling</i> layer is used to aggregate neighbouring point features using attention score for each feature. At the end these two layers will generate a informative feature vector with the most representative features and points. Finally the <i>Dilated residual block</i> will be in charge of stacking and propagating the features vector to neighboring points to increasing the receptive field that represent each point. A graphical representation of this process could be see it in the figure 5. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/receptiveFields.png\"\n",
    "         alt=\"rdnet layers\" \n",
    "         width=\"400\" \n",
    "         height=\"500\">\n",
    "    <figcaption>Figure 5. Dilated residual block, Global feature extraction</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Clustering \n",
    "\n",
    "To count the number of fruits, a clustering algorithm is used. The <i>DBSCAN</i> algorithm was chosen as it groups the segmented points in different clusters based on spatial density. \n",
    "\n",
    "DBSCAN is parameterized with two main elements: a distance threshold (<i>eps</i>) that specifies how close should be a group of points to be in the same cluster, and a minimum number of points to consider a region as dense.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "\n",
    "To install the fruithunter pipeline and its dependencies must be followed the foollowing steps:\n",
    "\n",
    "<ol>\n",
    "    <li> Create a virtual environment with <a href=\"https://docs.conda.io/\">conda</a> :<br>\n",
    "\n",
    "```bash\n",
    "conda create -n fruithunter  -c conda-forge -c anaconda python=3.6 scikit-learn scipy numpy pytest jupyter pandas matplotlib pcl cmake eigen boost tensorflow-gpu=1.13.1 PyYAML=5.1.2 cython=0.29.15 h5py=2.10.0\n",
    "```\n",
    "      \n",
    "  <li> Activate the conda environment <br>\n",
    "\n",
    "```bash\n",
    "conda activate fruithunter\n",
    "```\n",
    "      \n",
    "  <li> Install final dependency of Randla-NET <br>\n",
    "\n",
    "```bash\n",
    "pip install open3d-python==0.3.0\n",
    "```\n",
    "      \n",
    "  <li> Retrieve the source code of fruithunter:\n",
    "\n",
    "```bash\n",
    "git clone https://forgemia.inra.fr/openalea_phenotyping/fruithunter.git\n",
    "cd fruithunter/\n",
    "```\n",
    "\n",
    "  <li> Compile the package for the feature extraction (FPFH):\n",
    "      \n",
    "\n",
    "   - Create the build folder: <br>\n",
    "\n",
    "```bash\n",
    "mkdir pcl/build\n",
    "```\n",
    "   - Generate the cmake file: <br>\n",
    "\n",
    "```bash\n",
    "cd pcl/build; cmake .. ; make ; cd -\n",
    "```\n",
    "  <li> Get the Randla-NET repository: <br>\n",
    "\n",
    "```bash\n",
    "    git clone https://github.com/jrojas9206/RandLA-Net.git randlanet\n",
    "```\n",
    "     \n",
    "  <li> Compile the wrappers for Randla-NET:\n",
    "\n",
    "```bash\n",
    "cd randlanet/utils/cpp_wrappers ; sh compile_wrappers.sh ; cd -\n",
    "```\n",
    "      \n",
    "  <li> Install the nearest neighbors module:\n",
    "      \n",
    "```bash\n",
    "cd randlanet/utils/nearest_neighbors/ ; python setup.py install ; cd -\n",
    "```\n",
    "  <li> Install the pcl and randlanet modules:\n",
    "\n",
    "```bash\n",
    "python setup.py develop\n",
    "```\n",
    "      \n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic definitions \n",
    "In order to follow a simple order in this notebook, a generic python library will be imported and some paths to the data to be processed will be defined. Look the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the generic python libraries  with the following purposes \n",
    "# -> Handle the files\n",
    "import os\n",
    "import glob \n",
    "# -> Handle the arrays that represents the point clouds\n",
    "import numpy as np\n",
    "# -> To plot few interesting behaviours \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to datasets \n",
    "data2annotatedApples = \"data/example2notebook_realdata/\"\n",
    "data_fpfh = os.path.join(data2annotatedApples, \"fpfh\")\n",
    "dataRDnet = os.path.join(data2annotatedApples, \"data2randlanet\")\n",
    "classicSeg= os.path.join(data2annotatedApples, \"classicSegmentation_fpfh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_path_and_fileName(strPathName):\n",
    "    \"\"\"\n",
    "    Split the path and the name of the file of the given string \n",
    "    :INPUT:\n",
    "        strPathName: str with the path and name of a file. ex: /data/pointcloud.txt\n",
    "    :OUTPUT:\n",
    "        list, [\"/data/\", \"pointcloud.txt\"]\n",
    "    \"\"\"\n",
    "    for idx in range(len(strPathName)-1, 0, -1):\n",
    "        if(strPathName[idx]=='/'):\n",
    "            return strPathName[0:idx], strPathName[idx+1:]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from randlanet.utils.data_prepare_apple_tree import * \n",
    "\n",
    "def split_dataSet(path2files, path2output, model, verbose=False, protocol=\"field\"):\n",
    "    \"\"\"\n",
    "    Split and prepare the data on test and train for the implemented algorithms \n",
    "    Random Forest and RandLA-NET and \n",
    "    :INPUT:\n",
    "        path2files : str of the path to the folder of input files\n",
    "        path2output: str of the path to the output folder \n",
    "        model      : str, \"rdf\" or \"rdnet\"\n",
    "        verbose    : If true print few message of the code steps \n",
    "        protocol   : Type of protocol to handle ; synthetic/field/field_only_xyz\n",
    "    :OUTPUT:\n",
    "        Write the splitted dataset  on the folder\n",
    "    \"\"\"\n",
    "    # NOTE: This segment will be only executed from the notebook \n",
    "    lstOfFiles = glob.glob(os.path.join(path2files,\"*.txt\"))\n",
    "    if(verbose):\n",
    "        print(\"Found files: %i \" %(len(lstOfFiles)))\n",
    "    # Split the files\n",
    "    X_train, X_test, _,_ = train_test_split(lstOfFiles, range(len(lstOfFiles)), test_size=0.20, random_state=42)\n",
    "    if(verbose):\n",
    "        print(\" -> Train set: %i\" %len(X_train))\n",
    "        print(\" -> Test set : %i\" %len(X_test))\n",
    "    # Create the directory to keep the test and train sets \n",
    "    path2initialSplit = path2output #os.path.join(data2annotatedApples, \"dataToRDF\")\n",
    "    if(not os.path.isdir(path2initialSplit)):\n",
    "        os.mkdir(path2initialSplit)\n",
    "    for folderName, fileList in zip( [\"train\" if model == \"rdf\" else \"training\", \"test\"], [X_train, X_test] ):\n",
    "        path2saveData = os.path.join(path2initialSplit)\n",
    "        for file2feature in fileList:\n",
    "            output2wrt = os.path.join(path2saveData, folderName)\n",
    "            if(not os.path.isdir(output2wrt)):\n",
    "                os.mkdir(output2wrt)\n",
    "                print(\"Folder was created: %s\" %output2wrt)\n",
    "            print(\"-> Loading: %s\" %split_path_and_fileName(file2feature)[1])\n",
    "            file2wrt = os.path.join(output2wrt, split_path_and_fileName(file2feature)[1])\n",
    "            if(model == \"rdf\"):\n",
    "                # NOTE: If you change the position or the name of the feature generator change the\n",
    "                # next string \"cmd2feature\" [execution command]\n",
    "                cmd2features = \"./pcl/build/my_feature %s %.3f %s %s\" %(\"fpfh\",          # Feature extractor \n",
    "                                                                        0.025,           # Grid size \n",
    "                                                                        file2feature,    # Input File\n",
    "                                                                        file2wrt)        # Output File\n",
    "                print(\" -> Running feature extractor\")\n",
    "                os.system(cmd2features)\n",
    "            else: # RandLA-NET\n",
    "                if(folderName==\"test\"):\n",
    "                    convert_for_test(file2feature, path2saveData, grid_size=0.001, protocol=protocol)\n",
    "                else:\n",
    "                    convert_for_training(file2feature, None, path2saveData, grid_size=0.001, protocol=protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data \n",
    "The algorithms used, Random Forest and RandLA-NET, consume different representations of the point clouds and read different formats. Random Forest will use the features obtained from the FPFH to find the set of rules that gives the best classification of the apple's points. This script will load the point clouds from the \"txt\" files, and expect two type of order in the data for the training process, the first is XYZ+Annotations+FPFH, the second one is XYZ+Rediometric Features+Annotation+FPFH. And for the class prediction process the algorithm expects just the XYZ+FPFH. To get the desired features, two types of approaches can be followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to do this is by running the main script that generate the FPFH features. To run it, you have to locate the path \"fruithunter/pcl\" and inside of this folder execute the script called \"launcher.py\". An example of how execute this script is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will execute the python script, the first argument is the path where the point clouds are located \n",
    "# and the second argument is the folder where you want to put the output files. \n",
    "# Note 1: The below command was executed on the field measurements \n",
    "# Note 2: The following command will be executed as if it were executed from the terminal \n",
    "!python pcl/launcher.py data/example2notebook_realdata/ data/example2notebook_realdata/fpfh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We will execute the same command of above but to prepare the synthetic data \n",
    "!python pcl/launcher.py data/example2notebook_synthetic/ data/example2notebook_synthetic/fpfh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to execute the FPFH feature extraction after do some pre-processing you can do it by importing the respective module. In the following cell is shown the general code flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module that contain the FPFH feature extractor \n",
    "from pcl.launcher import launch_feature \n",
    "# Call the final method of the feature generation \n",
    "launch_feature(data2annotatedApples, data_fpfh)# This method will look for the txt files in the input path\n",
    "                                               # 'data2annotatedApples' It will write the featured files on \n",
    "                                               # the path 'data_fpfh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you verify the output path, you will see that new files have been created after execute the previous cells. This new files have the FPFH features. To see the differences between the files, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists with the names of the input and output files of the previous cells \n",
    "inputFiles  = glob.glob(os.path.join(data2annotatedApples,\"*.txt\"))\n",
    "outputFiles = glob.glob(os.path.join(data2annotatedApples, \"fpfh\",\"*.txt\")) \n",
    "lst_idx     = 0\n",
    "# Load a file and show their shape\n",
    "shape_inputFile = np.loadtxt(inputFiles[lst_idx]).shape\n",
    "shape_outputFile= np.loadtxt(outputFiles[lst_idx]).shape\n",
    "print(\"Input files: %i | Output files: %s\" %(len(inputFiles), len(outputFiles)))\n",
    "print(\"Shape input file: %s | Shape output file: %s\" %(str(shape_inputFile), str(shape_outputFile)))\n",
    "print(\"Size input file: %i[bytes] | Size output file: %s[bytes]\" %(os.path.getsize(inputFiles[lst_idx]), os.path.getsize(outputFiles[lst_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of randlanet the <i>txt</i> files have to be converted to <i>ply</i> format and their <i>kdtree</i> must be defined. On the other hand the model will consume the raw points (XYZ) or (XYZ+radiometric features). To clarify, in this case there is no need to estimate an internal vector of characteristics because the model finds the best characteristics that define the geometries by itself.<br>\n",
    "\n",
    "To do the file conversion could be followed two different ways. You can run the script \"utils/data_prepare_apple_tree.py\" or call the method on your own code. To see the process look the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the main script from terminal -- Training data \n",
    "!python utils/data_prepare_apple_tree.py data/example2notebook_realdata/toRandlanet/training/ --outputDir=data/example2notebook_realdata/toTrainRandLA_NET/training/ --datasetType=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the main script from terminal -- Test data\n",
    "# Note that when you set the argument datasetType to test the annotations in the outputfile will not appear \n",
    "!python utils/data_prepare_apple_tree.py data/example2notebook_realdata/toRandlanet/test/ --outputDir=data/example2notebook_realdata/toTrainRandLA_NET/ --datasetType=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the above task in your own code \n",
    "\n",
    "# Import the methods\n",
    "from randlanet.utils.data_prepare_apple_tree import * \n",
    "# We will execute the same behaviour of above but in the synthetic data --\n",
    "# for this we will create an extra variable\n",
    "path2Sdata         = \"data/example2notebook_synthetic/\"\n",
    "path2inputTraining = os.path.join(path2Sdata, \"toRandlanet/training/\")\n",
    "path2inputTest     = os.path.join(path2Sdata, \"toRandlanet/test/\")\n",
    "path2outpuT        = os.path.join(path2Sdata, \"toTrainRandLA_NET/\")\n",
    "# Execute the file conversion -- train \n",
    "prepare_data_generic(path2inputTraining, path2outpuT, grid_size=0.001, verbose=True, protocol=\"synthetic\", \n",
    "                     dataset=\"train\")\n",
    "# Execute the file conversion -- test\n",
    "prepare_data_generic(path2inputTest, path2outpuT, grid_size=0.001, verbose=True, protocol=\"synthetic\", \n",
    "                     dataset=\"test\")\n",
    "# Note: In the protocol argument you could define different options: \n",
    "#       -> synthetic: The data has the XYZ coordinate and the annotations \n",
    "#       -> field_only_xyz: The data has the XYZ coordinate and the annotations\n",
    "#       -> field: The data has the XYZ+radiometric+Annotations -- change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models \n",
    "\n",
    "We will train Random Forest and RandLA-NET to segment the apples in the real point clouds. The data to this test, could be found on the folder \"data/example2notebook_realdata\". In this folder, could be found 10 annotated real point clouds of apples trees. These point clouds have the XYZ coordinates, the radiometric features and the annotated apples. The steps to train the models are in the following cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest \n",
    "To train random forest, we are going to split our test in two main groups, \"train\" that will contain the 80% of the dataset and \"test\" that will have the final 20%. After do this splitting, we are going to calculate the FPFH features of each group of point clouds. And finally we will launch the trainning over the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell can be exectued only in the code \n",
    "out2files = \"data/example2notebook_realdata/dataToRDF/\"\n",
    "# This method will split and prepare the data \"features and desired format\"\n",
    "split_dataSet(data2annotatedApples, out2files, \"rdf\", verbose=True, protocol=\"field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest \n",
    "import myio\n",
    "# NOTE: The annotations on the field data are in the 7 position, but as we begin from 0, \n",
    "# we will said that is the 6 position  \n",
    "annotationColum  = 6\n",
    "# Set the paths to the test and train \n",
    "path2trainSet = os.path.join(out2files, \"train\")\n",
    "path2testSet  = os.path.join(out2files, \"test\")\n",
    "# Load the data \n",
    "train_set = myio.load_data(path2testSet)# NOTE: This method will load all the file on the referred folder\n",
    "                                        # it is possible that if you dont have enough RAM the process crash\n",
    "print(\"Original shape of the training set: %s\" %(str(train_set.shape)))\n",
    "# Get the annotation in a different array\n",
    "y_train = np.array([y[annotationColum] for y in train_set]) # Apple is annotated as 1 and other as 0\n",
    "print(\"Annotated points: %i\" %(len(y_train)))\n",
    "print(\"Found classes   : %s\" %(np.unique(y_train)))\n",
    "# Remove the annotation --\n",
    "train_set = np.delete(train_set, annotationColum, 1)\n",
    "# Remove the X Y Z coordinates and leave only the Features \n",
    "for _ in range(3):\n",
    "    train_set = np.delete(train_set, 0, 1)\n",
    "print(\"Final shape: %s\" %(str(train_set.shape))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning.RFClassifier import RFClassifier\n",
    "path2saveModel = os.path.join(data2annotatedApples, \"trained_rdf.sav\")\n",
    "# Train the random forest model \n",
    "model = RFClassifier()\n",
    "model.set_train_test_data(train_set, y_train, train_set, y_train)\n",
    "model.train()\n",
    "model.save(path2saveModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandLA-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data to randlanet \n",
    "out2files = \"data/example2notebook_realdata/dataToRDNET/\"\n",
    "split_dataSet(data2annotatedApples, out2files, \"rdnet\", verbose=True, protocol=\"field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"./utils\")\n",
    "from randlanet.main_apple_tree import *\n",
    "\n",
    "path2saveModel_r = os.path.join(data2annotatedApples, \"trained_randlanet\")\n",
    "\n",
    "param = {\"gpu\":0, # GPU ID\n",
    "         \"mode\":\"train\",  # to Predict\n",
    "         \"model_path\":None, \n",
    "         \"path2data\": out2files, \n",
    "         \"path2output\": path2saveModel_r, # This arg only works to save the training \n",
    "         \"protocol\":\"field_only_xyz\", \n",
    "         \"restoreTrain\":False}\n",
    "# RandLA-NET -- Under corrections -- Execute the script from Terminal\n",
    "train_field(out2files, path2saveModel_r, parameters=param)\n",
    "#train_field_only_xyz(out2files, path2saveModel_r, parameters=param)\n",
    "#train_synthetic_HiHiRes(out2files, path2saveModel_r, parameters=param)\n",
    "\n",
    "# NOTE: If a NoneType error is launched, verify that the point clouds are bigger \n",
    "# enough to the subsampling process, if not change the num_points variable in the script \n",
    "# helper_tool.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Clouds segmentation\n",
    "\n",
    "The segmentation of the fruits over the point clouds it is going to be approached using 3 different methods. The first one will use the FPFH to do a threshold and detect the apples. The second will use the Random Forest to classify the class of the points using the FPFH. Finally we will run the RandLA-NET model over the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data for the classic method \n",
    "files_lst = glob.glob(os.path.join(data_fpfh,\"*.txt\"))\n",
    "print(\"Found files: %i\" %(len(files_lst)))\n",
    "sampleCloud = np.loadtxt(files_lst[0])\n",
    "featuresIni = sampleCloud.shape[1]-1 # We remove 1 because of the annotations.\n",
    "                                     # Features on the file: \n",
    "                                     #   -> XYZ  [3]\n",
    "                                     #   -> Radiometric Values  [3] [Reflectance, Deviation, Amplitude]\n",
    "                                     #   -> FPFH [33]\n",
    "path2wrt = os.path.join(data_fpfh, \"noRadiometry/\")\n",
    "if(not os.path.isdir(path2wrt)):\n",
    "    os.mkdir(path2wrt)\n",
    "print(\"Features: %i\" %featuresIni)\n",
    "for fileNamePC in files_lst:\n",
    "    print(\"Loading file: %s\"%(split_path_and_fileName(fileNamePC)[1]))\n",
    "    a_pc = np.loadtxt(fileNamePC)\n",
    "    print(\"  -> Original shape: %s\" %(str(a_pc.shape)))\n",
    "    for i in range(5,2,-1): # Delete the radiometric columns  \n",
    "        a_pc = np.delete(a_pc, i, 1)\n",
    "    fname2wrt = path2wrt+split_path_and_fileName(fileNamePC)[1]\n",
    "    print(\"  -> Final shape: %s\" %(str(a_pc.shape)))\n",
    "    print(\"  -> File is going to  be wrote: %s \" %fname2wrt)\n",
    "    np.savetxt(fname2wrt, a_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pcl/launcher.py --action=1 data/example2notebook_realdata/fpfh/noRadiometry/ data/example2notebook_realdata/fpfh/segmentation_classic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal \n",
    "!mkdir data/example2notebook_realdata/dataToRDF/predicted/\n",
    "!python predict.py --model=data/example2notebook_realdata/trained_rdf.sav --path2data=data/example2notebook_realdata/dataToRDF/test/ --path2write=data/example2notebook_realdata/dataToRDF/predicted/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import synthetic_predict_rf\n",
    "# Code \n",
    "rdf_test = os.path.join(data2annotatedApples, \"dataToRDF/test/\")\n",
    "outputRdf = os.path.join(data2annotatedApples, \"dataToRDF/predicted/\")\n",
    "path2saveModel = os.path.join(data2annotatedApples, \"trained_rdf.sav\")\n",
    "# Random Forest -- notebook \n",
    "synthetic_predict_rf(path2saveModel, rdf_test, outputRdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandLA-NET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal \n",
    "!python utils/main_apple_tree.py --mode=test --model_path=data/example2notebook_realdata/trained_randlanet/snapshots/snap-XXX  --inputDir=data/example2notebook_realdata/dataToRDNET/\n",
    "#NOTE: Change the XXX for the name of the last available snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code \n",
    "import sys \n",
    "sys.path.append(\"./utils/\")\n",
    "from randlanet.main_apple_tree import *\n",
    "from randlanet.utils.merge_label_apple import *\n",
    "# Arguments for the model\n",
    "param = {\"gpu\":0, # GPU ID\n",
    "         \"mode\":\"test\",  # to Predict\n",
    "         \"model_path\":path2saveModel_r, \n",
    "         \"path2data\": out2files, \n",
    "         \"path2output\": \"./\", # This arg only works to save the training \n",
    "         \"protocol\":\"field_only_xyz\", \n",
    "         \"restoreTrain\":True}\n",
    "# Predict\n",
    "if(param[\"protocol\"] == \"synthetic\"):\n",
    "    train_synthetic_HiHiRes(param[\"path2data\"], param[\"path2output\"], parameters=param)\n",
    "elif(param[\"protocol\"] == \"field_only_xyz\"):\n",
    "    train_field_only_xyz(param[\"path2data\"], param[\"path2output\"], parameters=param)\n",
    "elif(param[\"protocol\"] == \"field\"):\n",
    "    train_field(param[\"path2data\"], param[\"path2output\"], parameters=param)\n",
    "else:\n",
    "    print(\"-> Error: Unknow option\")\n",
    "# Merge label\n",
    "path2prediction = \"test/Log_XXXX-XX-XX_XX-XX-XX/predictions/\" # Change the XXXX for the respective date \n",
    "                                            \n",
    "path2data = os.path.join(param[\"path2data\"],\"test/\")\n",
    "output = os.path.join(out2files, \"prediction/\")\n",
    "merge_pointCloudAndLabels(path2data, \"./test/\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Counting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = 0\n",
    "files_annApples = glob.glob(os.path.join(data2annotatedApples, \"output_rdf_prediction\", \"*.txt\"))\n",
    "path, fname = split_path_and_fileName(files_annApples[file_index])\n",
    "fname = \"cluster_%s\"%(fname)\n",
    "# Load the processed file \n",
    "output = os.path.join(data2annotatedApples, \"output_rdf_prediction\",fname)\n",
    "print(\"Cluster: %s\" %(output))\n",
    "min_samples = 50\n",
    "eps = 0.04\n",
    "pcPrediction = np.loadtxt(files_annApples[0])\n",
    "# Get the clusters using the predicted labels \n",
    "clPrediction = clustering(pcPrediction, min_samples, eps)\n",
    "# Write the cluster \n",
    "np.savetxt(output, clPrediction) # To visualize the clusti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_processing.algorithm import clustering \n",
    "eps, minSamples = 0.04, 50\n",
    "\n",
    "path2write = os.path.join(outputRdf, \"clustering/\")\n",
    "\n",
    "if(not os.path.isdir(path2write)):\n",
    "    os.mkdir(path2write)\n",
    "    \n",
    "files_annApples = glob.glob(os.path.join(outputRdf,\"*.txt\"))\n",
    "print(\"Found annotated files: %i\" %(len(files_annApples)))\n",
    "for idx, file2clustering in enumerate(files_annApples, start=1):\n",
    "    _, actualFileName = split_path_and_fileName(file2clustering)\n",
    "    print(\"-> Loading[%i/%i]: %s\" %(idx, len(files_annApples), actualFileName))\n",
    "    pointCloud2cluster = np.loadtxt(file2clustering)\n",
    "    cluster = clustering(pointCloud2cluster, minSamples, eps)\n",
    "    print(\" -> The file will be written in: %s\" %path2write)\n",
    "    np.savetxt(path2write+actualFileName, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the apples  from the generated clusters \n",
    "clusteredClouds = glob.glob(os.path.join(path2write,\"*.txt\"))\n",
    "colOfClusters   = 4 # The cluster function will add at the end of the predicted files a column with the clusters\n",
    "                    # and after it will write the file with the new column in the specified folder\n",
    "print(\"Files: %i\" %((len(clusteredClouds))))\n",
    "for fileName in clusteredClouds:\n",
    "    _, actualFileName = split_path_and_fileName(file2clustering)\n",
    "    pc = np.loadtxt(fileName)\n",
    "    apples = len(np.unique(pc[:,colOfClusters]))\n",
    "    print(\"File: %s\" %actualFileName)\n",
    "    print(\" -> Found apples[clusters]: %i\" %apples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Hu, Qingyong, et al. \"RandLA-Net: Efficient semantic segmentation of large-scale point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. <br>\n",
    "[2] Pedregosa, Fabian, et al. \"Scikit-learn: Machine learning in Python.\" the Journal of machine Learning research 12 (2011): 2825-2830.<br>\n",
    "[3] Raschka, Sebastian, and Vahid Mirjalili. Python machine learning. Packt Publishing Ltd, 2017.<br>\n",
    "[4] Rusu, R. B., and S. Cousins. \"3D is here: point cloud library.\" Point Cloud Library http://pointclouds.org/. Accessed January 8 (2021). <br>\n",
    "[5] Schubert, Erich, et al. \"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\" ACM Transactions on Database Systems (TODS) 42.3 (2017): 1-21.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
