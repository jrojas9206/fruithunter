{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "labeled-thunder",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the data, and restore their original structure \n",
    "# When you exectue this script, it will create the following structure \n",
    "# data\n",
    "#  - merged_xyz_radiometric_Clusters_Annotations\n",
    "#    - *.txt  // Each text file contain XYZ+Radiometric+Cluster+Annotations\n",
    "#    - original\n",
    "#      - *.txt // Each txt file contain the xyz+radiometric+annotations\n",
    "#    - cluster \n",
    "#      - *.txt // Each txt file contain the xyz+annotations+clusters \n",
    "#    - annotations  \n",
    "#      - *.txt // Each txt file contain the XYZ+annotations \n",
    "#  - model_RF-field_fpfh\n",
    "#    - model_all.sav\n",
    "#    - model_fold_x.sav where x is {1,2,3,4,5}\n",
    "#    - learning.log\n",
    "#  - model_RF-field_rad_fpfh\n",
    "#    - model_fold_x.sav where x is {1,2}\n",
    "#    - learning.log \n",
    "#  - randlanet_field_and_fieldOnlyXYZ\n",
    "#    - model_RandLA-Net_field\n",
    "#      - snapshots\n",
    "#        - snap-XXXX \n",
    "#    - model_RandLA-Net_field_only_xyz\n",
    "#      - snapshots\n",
    "#        - snap-XXXX \n",
    "!python restoreData.py --action=uncompress --path2tar=../data.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-lindsay",
   "metadata": {},
   "source": [
    "## Prepare for RandLA-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from restoreData import dataSet\n",
    "# Path to the previous generated files \n",
    "p_path2data_o = os.path.join(\"../data/merged_xyz_radiometric_Clusters_Annotations/original/\")\n",
    "p_path2data_a = os.path.join(\"../data/merged_xyz_radiometric_Clusters_Annotations/annotations/\")\n",
    "# Output path for the prepared data to randlanet \n",
    "p2rnet_out_o = os.path.join(p_path2data_o, \"data2rnet\")\n",
    "p2rnet_out_a = os.path.join(p_path2data_a, \"data2rnet\")\n",
    "if(not os.path.isdir(p2rnet_out_o)):\n",
    "    os.mkdir(p2rnet_out_o)\n",
    "if(not os.path.isdir(p2rnet_out_a)):\n",
    "    os.mkdir(p2rnet_out_a)\n",
    "# Set them in the required format \n",
    "print(\"-> Original: XYZ+Radiometric+Annotations\")\n",
    "dataSet(p_path2data_o, p2rnet_out_o, \"rnet\", verbose=True, protocol=\"field\")\n",
    "print(\"-> Only XYZ: XYZ+Annotations\")\n",
    "dataSet(p_path2data_a, p2rnet_out_a, \"rnet\", verbose=True, protocol=\"field_only_xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-principle",
   "metadata": {},
   "source": [
    "## Prepare data for Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from restoreData import dataSet\n",
    "# Path to the previous generated files \n",
    "p_path2data_o = os.path.join(\"../data/merged_xyz_radiometric_Clusters_Annotations/original/\")\n",
    "p_path2data_a = os.path.join(\"../data/merged_xyz_radiometric_Clusters_Annotations/annotations/\")\n",
    "# Output path for the prepared data to randlanet \n",
    "p2rf_out_o = os.path.join(p_path2data_o, \"data2rf\")\n",
    "p2rf_out_a = os.path.join(p_path2data_a, \"data2rf\")\n",
    "if(not os.path.isdir(p2rf_out_o)):\n",
    "    os.mkdir(p2rf_out_o)\n",
    "if(not os.path.isdir(p2rf_out_a)):\n",
    "    os.mkdir(p2rf_out_a)\n",
    "# Set them in the required format \n",
    "print(\"-> Original: XYZ+Radiometric+Annotations\")\n",
    "dataSet(p_path2data_o, p2rf_out_o, \"rdf\", verbose=True)\n",
    "print(\"-> Only XYZ: XYZ+Annotations\")\n",
    "dataSet(p_path2data_a, p2rf_out_a, \"rdf\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-brand",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-supply",
   "metadata": {},
   "source": [
    "## RandLA-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "from randlanet.main_apple_tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2model_rnet= \"../data/randlanet_field_and_fieldOnlyXYZ/model_RandLA-Net_field_only_xyz/snapshots/snap-13001\" # Trained model\n",
    "path2data = \"../data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rnet\" # Data to randlanet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Input path: %s\" %(\"Not found\" if not os.path.isdir(path2data) else \"OK\" ) )\n",
    "print(\"-> Model path: %s\" %(\"Not found\" if not os.path.isfile(path2model_rnet+\".meta\") else \"OK\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for the model\n",
    "param = {\"gpu\":0, # -1 no GPU\n",
    "         \"model_path\":path2model_rnet, \n",
    "         \"path2data\":path2data, \n",
    "         \"path2output\": \"./\", # This arg only works to save the training \n",
    "         \"protocol\":\"field_only_xyz\", \n",
    "         \"trainFromCHK\":False}  \n",
    "# NOTE: Ensure that the subsampling points in the training are the same for the prediction[test] to do this verify\n",
    "# the file called helper_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "tf.reset_default_graph() # Ensure that the models is not being reused by any previous call \n",
    "\n",
    "randlanet_predict(param)\n",
    "# When this metod is called, a folder called test is going to bre created and inside of this folder, is related\n",
    "# the folder called prediction thatn contain *.labels files with the class of each point.\n",
    "\n",
    "# NOTE: If you have a memory problem try to reduce the number of points in the subsampling [helper_tools.py] and also reduce \n",
    "# the batch size. Note that changing the number of point for the prediction could affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels \n",
    "# NOTE: RandLA-NET write the probabilities and the labels of each point cloud on different files, \n",
    "# To visualize the classification the predicted classes and the point cloud are going to be merged \n",
    "from randlanet.utils.merge_label_apple import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2prediction = \"test/Log_XXXXX/predictions/\" # The name of the folder always is \n",
    "                                                # going to change with the date\n",
    "path2data = os.path.join(param[\"path2data\"],\"test/\")\n",
    "OutputPath = os.path.join(path2data, \"merged/\")\n",
    "\n",
    "merge_pointCloudAndLabels(path2data, \"./test/\", OutputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-payday",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import numpy as np \n",
    "from machine_learning.RFClassifier import RFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RFClassifier()\n",
    "#\n",
    "model_weights = \"../data/model_RF-field_fpfh/model_fold_1.sav\"\n",
    "path2data = \"../data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/\"\n",
    "#\n",
    "OutputPath = os.path.join(path2data, \"prediction_rf/\")\n",
    "if(not os.path.isdir(OutputPath)):\n",
    "    os.mkdir(OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-uncertainty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model \n",
    "model.load(model_weights)\n",
    "P = 0.7\n",
    "#\n",
    "filedOnlyXYZ = True \n",
    "#\n",
    "lst_f2rf = glob.glob(os.path.join(path2data,\"*.txt\"))\n",
    "for idx, fname in enumerate(lst_f2rf, start=1):\n",
    "    print(\"-> Loading[%i/%i]: %s\" %( len(lst_f2rf), idx, os.path.split(fname)[1] ))\n",
    "    pc2rf = np.loadtxt(fname)\n",
    "    xyz   = pc2rf[:,0:3].reshape(-1,3)\n",
    "    print(\" -> shape: %s\" %(str(pc2rf.shape)))\n",
    "    # Remove XYZ and the annotations \n",
    "    if(filedOnlyXYZ):\n",
    "        pc2rf = np.delete(pc2rf, [0,1,2,3], axis=1)# Delete XYZ Annotations \n",
    "        print(\"   -> New shape: %s\" %(str(pc2rf.shape)))\n",
    "    else: # the point cloud has the radiometric features XYZ+Radiometric+Annotations \n",
    "        pc2rf = np.delete(pc2rf, [0,1,2,6], axis=1) # Delete XYZ+radiom\n",
    "        print(\"   -> New shape: %s\" %(str(pc2rf.shape)))\n",
    "    print(\" -> Classifing points\")\n",
    "    predicted = model.predict(pc2rf)\n",
    "    predicted = np.where(predicted>P, 1, 0)\n",
    "    print(\"   -> OK\")\n",
    "    predicted = np.concatenate((xyz, predicted.reshape(-1,1)), axis=1)\n",
    "    p2save = os.path.join(OutputPath, os.path.split(fname)[1])\n",
    "    np.savetxt(p2save, predicted)\n",
    "#predict(model, model_weights, path2data, path2output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-mills",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "from machine_learning.ModelClassifier import ModelClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "isRnet = False\n",
    "lst_predictions = glob.glob(os.path.join(OutputPath,\"*.labels\" if isRnet else \"*.txt\"))\n",
    "mc = ModelClassifier()\n",
    "for idx, i in enumerate(lst_predictions, start=1):\n",
    "    print(\"Loading[%i/%i]: %s\" %(len(lst_predictions), idx, os.path.split(i)[1]))\n",
    "    p2p = os.path.join(OutputPath, os.path.split(i)[1])\n",
    "    p2g = os.path.join(path2data, os.path.split(i)[1])\n",
    "    if(isRnet):\n",
    "        pcp_lbl = np.loadtxt(p2p)\n",
    "    else:\n",
    "        pcp_lbl = np.loadtxt(p2p)[:,-1]\n",
    "    pcg_lbl = np.loadtxt(p2g)[:,-1 if isRnet else 3]\n",
    "    mc.evaluate(pcg_lbl, pcp_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-investigator",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import numpy as np \n",
    "import sklearn.cluster\n",
    "from post_processing.algorithm import clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_annApples = glob.glob(os.path.join(OutputPath,\"*.txt\"))\n",
    "path2wrt = os.path.join(OutputPath,\"clusters/\")\n",
    "\n",
    "if(not os.path.join(path2wrt)):\n",
    "    os.mkdir(path2wrt)\n",
    "\n",
    "eps, minSamples = 0.1, 20 # 0.4, 20 funciona pero consume mucha memoria \n",
    "\n",
    "print(\"Found annotated files: %i\" %(len(files_annApples)))\n",
    "\n",
    "for idx, file2clustering in enumerate(files_annApples, start=1):\n",
    "    _, actualFileName = os.path.split(file2clustering)\n",
    "    print(\"-> Loading[%i/%i]: %s\" %(len(files_annApples), idx, actualFileName))\n",
    "    pointCloud2cluster = np.loadtxt(file2clustering)\n",
    "    cluster = clustering(pointCloud2cluster, minSamples, eps)\n",
    "    print(\" -> The file will be written in: %s\" %path2wrt)\n",
    "    np.savetxt(path2wrt+actualFileName, cluster)# The cluster is in the last column of the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
