{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "green-tissue",
   "metadata": {},
   "source": [
    "# Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from randlanet.utils.data_prepare_apple_tree import * \n",
    "from pcl.launcher import launch_feature\n",
    "def dataSet(path2files, path2output, model, verbose=False, protocol=\"field\"):\n",
    "    \"\"\"\n",
    "    :INPUT:\n",
    "        path2files : str of the path to the folder of input files\n",
    "        path2output: str of the path to the output folder \n",
    "        model      : str, \"rdf\" or \"rdnet\"\n",
    "        verbose    : If true print few message of the code steps \n",
    "        protocol   : Type of protocol to handle ; synthetic/field/field_only_xyz\n",
    "    :OUTPUT:\n",
    "        Write the splitted dataset  on the folder\n",
    "    \"\"\"\n",
    "    # NOTE: This segment will be only executed from the notebook \n",
    "    lstOfFiles = glob.glob(os.path.join(path2files,\"*.txt\"))\n",
    "    if(verbose):\n",
    "        print(\"Found files: %i \" %(len(lstOfFiles)))\n",
    "    # Split the files\n",
    "    X_train, X_test, _,_ = train_test_split(lstOfFiles, range(len(lstOfFiles)), test_size=0.20, random_state=42)\n",
    "    if(verbose):\n",
    "        print(\" -> Train set: %i\" %len(X_train))\n",
    "        print(\" -> Test set : %i\" %len(X_test))\n",
    "    # Create the directory to keep the test and train sets \n",
    "    path2initialSplit = path2output #os.path.join(data2annotatedApples, \"dataToRDF\")\n",
    "    if(not os.path.isdir(path2initialSplit)):\n",
    "        os.mkdir(path2initialSplit)\n",
    "    lst_folders = []\n",
    "    if(model==\"rdf\"):\n",
    "        lst_folders = [\"test\", \"train\"]\n",
    "    elif(model==\"rnet\"):\n",
    "        lst_folders = [\"test\", \"training\"]\n",
    "    else:\n",
    "        return -1\n",
    "    for folderName, fileList in zip( lst_folders,[X_test, X_train]):\n",
    "        path2saveData = os.path.join(path2initialSplit)\n",
    "        for file2feature in fileList:\n",
    "            output2wrt = os.path.join(path2saveData, folderName)\n",
    "            if(not os.path.isdir(output2wrt)):\n",
    "                os.mkdir(output2wrt)\n",
    "                print(\"Folder was created: %s\" %output2wrt)\n",
    "            print(\"-> Loading: %s\" %os.path.split(file2feature)[1])\n",
    "            file2wrt = os.path.join(output2wrt, os.path.split(file2feature)[1])\n",
    "            if(model == \"rdf\"):\n",
    "                # NOTE: If you change the position or the name of the feature generator change the\n",
    "                # next string \"cmd2feature\" [execution command]\n",
    "                cmd2features = \"./../pcl/build/my_feature %s %.3f %s %s\" %(\"fpfh\",          # Feature extractor \n",
    "                                                                        0.025,           # Grid size \n",
    "                                                                        file2feature,    # Input File\n",
    "                                                                        file2wrt)        # Output File\n",
    "                print(\" -> Running feature extractor\")\n",
    "                os.system(cmd2features)\n",
    "            else: # RandLA-NET\n",
    "                if(folderName==\"test\"):\n",
    "                    convert_for_test(file2feature, path2saveData, grid_size=0.001, protocol=protocol)\n",
    "                else:\n",
    "                    convert_for_training(file2feature, None, path2saveData, grid_size=0.001, protocol=protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-thunder",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the data, and restore their original structure \n",
    "# When you exectue this script, it will create the following structure \n",
    "# data\n",
    "#  - merged_xyz_radiometric_Clusters_Annotations\n",
    "#    - *.txt  // Each text file contain XYZ+Radiometric+Cluster+Annotations\n",
    "#    - original\n",
    "#      - *.txt // Each txt file contain the xyz+radiometric+annotations\n",
    "#    - cluster \n",
    "#      - *.txt // Each txt file contain the xyz+annotations+clusters \n",
    "#    - annotations  \n",
    "#      - *.txt // Each txt file contain the XYZ+annotations \n",
    "#  - model_RF-field_fpfh\n",
    "#    - model_all.sav\n",
    "#    - model_fold_x.sav where x is {1,2,3,4,5}\n",
    "#    - learning.log\n",
    "#  - model_RF-field_rad_fpfh\n",
    "#    - model_fold_x.sav where x is {1,2}\n",
    "#    - learning.log \n",
    "#  - randlanet_field_and_fieldOnlyXYZ\n",
    "#    - model_RandLA-Net_field\n",
    "#      - snapshots\n",
    "#        - snap-XXXX \n",
    "#    - model_RandLA-Net_field_only_xyz\n",
    "#      - snapshots\n",
    "#        - snap-XXXX \n",
    "!python restoreData.py --action=uncompress --path2tar=./data.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-lindsay",
   "metadata": {},
   "source": [
    "## Prepare for RandLA-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# Path to the previous generated files \n",
    "p_path2data_o = os.path.join(\"data/merged_xyz_radiometric_Clusters_Annotations/original/\")\n",
    "p_path2data_a = os.path.join(\"data/merged_xyz_radiometric_Clusters_Annotations/annotations/\")\n",
    "# Output path for the prepared data to randlanet \n",
    "p2rnet_out_o = os.path.join(p_path2data_o, \"data2rnet\")\n",
    "p2rnet_out_a = os.path.join(p_path2data_a, \"data2rnet\")\n",
    "if(not os.path.isdir(p2rnet_out_o)):\n",
    "    os.mkdir(p2rnet_out_o)\n",
    "if(not os.path.isdir(p2rnet_out_a)):\n",
    "    os.mkdir(p2rnet_out_a)\n",
    "# Set them in the required format \n",
    "print(\"-> Original: XYZ+Radiometric+Annotations\")\n",
    "dataSet(p_path2data_o, p2rnet_out_o, \"rnet\", verbose=True, protocol=\"field\")\n",
    "print(\"-> Only XYZ: XYZ+Annotations\")\n",
    "dataSet(p_path2data_a, p2rnet_out_a, \"rnet\", verbose=True, protocol=\"field_only_xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-principle",
   "metadata": {},
   "source": [
    "## Prepare data for Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# Path to the previous generated files \n",
    "p_path2data_o = os.path.join(\"data/merged_xyz_radiometric_Clusters_Annotations/original/\")\n",
    "p_path2data_a = os.path.join(\"data/merged_xyz_radiometric_Clusters_Annotations/annotations/\")\n",
    "# Output path for the prepared data to randlanet \n",
    "p2rf_out_o = os.path.join(p_path2data_o, \"data2rf\")\n",
    "p2rf_out_a = os.path.join(p_path2data_a, \"data2rf\")\n",
    "if(not os.path.isdir(p2rf_out_o)):\n",
    "    os.mkdir(p2rf_out_o)\n",
    "if(not os.path.isdir(p2rf_out_a)):\n",
    "    os.mkdir(p2rf_out_a)\n",
    "# Set them in the required format \n",
    "print(\"-> Original: XYZ+Radiometric+Annotations\")\n",
    "dataSet(p_path2data_o, p2rf_out_o, \"rdf\", verbose=True)\n",
    "print(\"-> Only XYZ: XYZ+Annotations\")\n",
    "dataSet(p_path2data_a, p2rf_out_a, \"rdf\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-brand",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-supply",
   "metadata": {},
   "source": [
    "## RandLA-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "from randlanet.main_apple_tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2model_rnet= \"data/randlanet_field_and_fieldOnlyXYZ/model_RandLA-Net_field_only_xyz/snapshots/snap-13001\" # Trained model\n",
    "path2data = \"data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rnet\" # Data to randlanet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Input path: %s\" %(\"Not found\" if not os.path.isdir(path2data) else \"OK\" ) )\n",
    "print(\"-> Model path: %s\" %(\"Not found\" if not os.path.isfile(path2model_rnet+\".meta\") else \"OK\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for the model\n",
    "param = {\"gpu\":0, # -1 no GPU\n",
    "         \"model_path\":path2model_rnet, \n",
    "         \"path2data\":path2data, \n",
    "         \"path2output\": \"./\", # This arg only works to save the training \n",
    "         \"protocol\":\"field_only_xyz\", \n",
    "         \"trainFromCHK\":False}  \n",
    "# NOTE: Ensure that the subsampling points in the training are the same for the prediction[test] to do this verify\n",
    "# the file called helper_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "tf.reset_default_graph() # Ensure that the models is not being reused by any previous call \n",
    "\n",
    "randlanet_predict(param)\n",
    "# When this metod is called, a folder called test is going to bre created and inside of this folder, is related\n",
    "# the folder called prediction thatn contain *.labels files with the class of each point.\n",
    "\n",
    "# NOTE: If you have a memory problem try to reduce the number of points in the subsampling [helper_tools.py] and also reduce \n",
    "# the batch size. Note that changing the number of point for the prediction could affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge labels \n",
    "# NOTE: RandLA-NET write the probabilities and the labels of each point cloud on different files, \n",
    "# To visualize the classification the predicted classes and the point cloud are going to be merged \n",
    "from randlanet.utils.merge_label_apple import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2prediction = \"test/Log_XXXXX/predictions/\" # The name of the folder always is \n",
    "                                                # going to change with the date\n",
    "path2data = os.path.join(param[\"path2data\"],\"test/\")\n",
    "OutputPath = os.path.join(path2data, \"merged/\")\n",
    "\n",
    "merge_pointCloudAndLabels(path2data, \"./test/\", OutputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-payday",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stable-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import numpy as np \n",
    "from machine_learning.RFClassifier import RFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "difficult-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RFClassifier()\n",
    "#\n",
    "model_weights = \"data/model_RF-field_fpfh/model_fold_1.sav\"\n",
    "path2data = \"data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/\"\n",
    "#\n",
    "OutputPath = os.path.join(path2data, \"prediction_rf/\")\n",
    "if(not os.path.isdir(OutputPath)):\n",
    "    os.mkdir(OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-uncertainty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model \n",
    "model.load(model_weights)\n",
    "P = 0.7\n",
    "#\n",
    "filedOnlyXYZ = True \n",
    "#\n",
    "lst_f2rf = glob.glob(os.path.join(path2data,\"*.txt\"))\n",
    "for idx, fname in enumerate(lst_f2rf, start=1):\n",
    "    print(\"-> Loading[%i/%i]: %s\" %( len(lst_f2rf), idx, os.path.split(fname)[1] ))\n",
    "    pc2rf = np.loadtxt(fname)\n",
    "    xyz   = pc2rf[:,0:3].reshape(-1,3)\n",
    "    print(\" -> shape: %s\" %(str(pc2rf.shape)))\n",
    "    # Remove XYZ and the annotations \n",
    "    if(filedOnlyXYZ):\n",
    "        pc2rf = np.delete(pc2rf, [0,1,2,3], axis=1)# Delete XYZ Annotations \n",
    "        print(\"   -> New shape: %s\" %(str(pc2rf.shape)))\n",
    "    else: # the point cloud has the radiometric features XYZ+Radiometric+Annotations \n",
    "        pc2rf = np.delete(pc2rf, [0,1,2,6], axis=1) # Delete XYZ+radiom\n",
    "        print(\"   -> New shape: %s\" %(str(pc2rf.shape)))\n",
    "    print(\" -> Classifing points\")\n",
    "    predicted = model.predict(pc2rf)\n",
    "    predicted = np.where(predicted>P, 1, 0)\n",
    "    print(\"   -> OK\")\n",
    "    predicted = np.concatenate((xyz, predicted.reshape(-1,1)), axis=1)\n",
    "    p2save = os.path.join(OutputPath, os.path.split(fname)[1])\n",
    "    np.savetxt(p2save, predicted)\n",
    "#predict(model, model_weights, path2data, path2output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-mills",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perfect-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "from machine_learning.ModelClassifier import ModelClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dynamic-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading[2/1]: tree_2018_08_02_L03_P16_filtered_labeled_noFloor.txt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/prediction_rf/data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/prediction_rf/tree_2018_08_02_L03_P16_filtered_labeled_noFloor.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c86d65142970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpcp_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpcp_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpcg_lbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misRnet\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misRnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/76db166d-2c59-4b55-9a91-19935005e2ef/miniconda3/envs/fh/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/76db166d-2c59-4b55-9a91-19935005e2ef/miniconda3/envs/fh/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/76db166d-2c59-4b55-9a91-19935005e2ef/miniconda3/envs/fh/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/prediction_rf/data/merged_xyz_radiometric_Clusters_Annotations/annotations/data2rf/test/prediction_rf/tree_2018_08_02_L03_P16_filtered_labeled_noFloor.txt not found."
     ]
    }
   ],
   "source": [
    "isRnet = False\n",
    "lst_predictions = glob.glob(os.path.join(OutputPath,\"*.labels\" if isRnet else \"*.txt\"))\n",
    "mc = ModelClassifier()\n",
    "for idx, i in enumerate(lst_predictions, start=1):\n",
    "    print(\"Loading[%i/%i]: %s\" %(len(lst_predictions), idx, os.path.split(i)[1]))\n",
    "    p2p = os.path.join(OutputPath, os.path.split(i)[1])\n",
    "    p2g = os.path.join(path2data, os.path.split(i)[1])\n",
    "    if(isRnet):\n",
    "        pcp_lbl = np.loadtxt(p2p)\n",
    "    else:\n",
    "        pcp_lbl = np.loadtxt(p2p)[-1]\n",
    "    pcg_lbl = np.loadtxt(p2g)[-1 if isRnet else 3]\n",
    "    if(not isRnet):\n",
    "        pcp_lbl = pcp_lbl[:,3] # The classification of random forest is \n",
    "                             # always on the last column [3] of the generated file\n",
    "        mc.evaluate(pcg_lbl, pcp_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-investigator",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import numpy as np \n",
    "import sklearn.cluster\n",
    "from post_processing.algorithm import clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_annApples = glob.glob(os.path.join(OutputPath,\"*.txt\"))\n",
    "path2wrt = os.path.join(OutputPath,\"clusters/\")\n",
    "\n",
    "if(not os.path.join(path2wrt)):\n",
    "    os.mkdir(path2wrt)\n",
    "\n",
    "eps, minSamples = 0.1, 20 # 0.4, 20 funciona pero consume mucha memoria \n",
    "\n",
    "print(\"Found annotated files: %i\" %(len(files_annApples)))\n",
    "\n",
    "for idx, file2clustering in enumerate(files_annApples, start=1):\n",
    "    _, actualFileName = os.path.split(file2clustering)\n",
    "    print(\"-> Loading[%i/%i]: %s\" %(len(files_annApples), idx, actualFileName))\n",
    "    pointCloud2cluster = np.loadtxt(file2clustering)\n",
    "    cluster = clustering(pointCloud2cluster, minSamples, eps)\n",
    "    print(\" -> The file will be written in: %s\" %path2wrt)\n",
    "    np.savetxt(path2wrt+actualFileName, cluster)# The cluster is in the last column of the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
